---
layout: post
title: Reinforcement Learning Study 1
date: 2021-01-29 11:10:00 +0900
category: Study 
---
#### Textbook: Reinforcement Learning: An Introduction - Sutton and Barto

# Chapter 1. Introduction
> _Reinforcement learning (RL) is learning what to do so as to maximize the numerical reward signal._

+ Two distinguishing features of RL = Trial-and-Error + Delayed Reward

> _A learning agent must be able to sense the state of its environment to some extent and must be able to take actions that affect the state. The agent also must have a goal or goals relating to the state of the environment._

```ruby
state, reward = Environment(action)
action = agent(state, reward, goal)
```

+ _RL is different from SL_: No examples of desired behavior that are both correct and representative of all the situations.

+ _RL is different from UL_: Maximizing a reward signal but not uncovering the structure.

+ Exploitation of experience vs Exploration of not selected actions

+ RL's subelememts: _policy_, _reward_, _value function_, _model_ (_planning_)


# Chapter 2. Multi-armed Bandits
> _k-armed bandit problem: You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period._

+ Expected Reward ($q_{\star}$)

$$ 
{\small
q_{\star}(a)=E[R_{t}|A_{t}=a] 
}
$$

+ Estimated Value of action $a$ at $t$ ($Q_{t}(a)$)

$$
{\small
Q_{t}(a) = \frac{\text{sum of rewards when }a\text{ taken prior to }t}{\text{number of times }a\text{ taken prior to }t}
}
$$

+ _Greedy_ Action Selection

$$
{\small
A_{t}=\underset{a}{\text{argmax}} Q_{t}(a)
}
$$

+ $\epsilon-$_Greedy_ Action Selection

```ruby
if rand() > epsilon:
	action = argmax(Qt)
else:
	action = randint(numAction)
```

+ 10 Armed Bandit Problem

![](/Figs/RL_Sutton/Ch2/10ArmedBandit.jpg){: width="100%" height="100%"}


+ Performance of $\epsilon-$_Greedy_ Action 

![](/Figs/RL_Sutton/Ch2/EpsGreedy.jpg){: width="100%" height="100%"}


+ Incremental Implementation
$$
{\small
NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]
}
$$

$$
{\small
Q_{n} = \frac{R_{1}+R_{2}+\cdots+R_{n-1}}{n-1}
Q_{n+1}=Q_{n}+\frac{1}{n}[R_{n}-Q_{n}]
}
$$

+ Incremental Implementation for Nonstationary Problem

$$
{\small
Q_{n+1}=Q_{n}+\alpha[R_{n}-Q_{n}] 
       =(1-\alpha)^{n}Q_{1}+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-i}R_{i}
}
$$


+ Optimistic Initial Values

![](/Figs/RL_Sutton/Ch2/OptInitial.jpg){: width="100%" height="100%"}


+ Upper-Confidence-Bound (UCB) Action Selection

$$
{\small
A_{t}=\underset{a}{\text{argmax}} [Q_{t}(a)+c\sqrt{\frac{\log{t}}{N_{t}(a)}}]
}
$$

![](/Figs/RL_Sutton/Ch2/UCB.jpg){: width="100%" height="100%"}

+ Gradient Bandit Algorithm

$$
{\small
\Pr\{A_{t}=a\}=\frac{e^{H_{t}(a)}}{\sum_{b=1}^{k}e^{H_{t}(b)}}=\pi_{t}(a)
}
\\
{\small
H_{t+1}(A_{t})=H_{t}(A_{t})+\alpha(R_{t}-\bar{R}_{t})(1-\pi_{t}(A_{t}))} \\
{\small
H_{t+1}(a)=H_{t}(a)-\alpha(R_{t}-\bar{R}_{t})\pi_{t}(a),   {\forall}a{\neq}A_{t} 
}
$$
