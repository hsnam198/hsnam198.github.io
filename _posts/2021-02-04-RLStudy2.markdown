---
layout: post
title: Reinforcement Learning Study 2
date: 2021-02-04 11:10:00 +0900
category: Study 
---
#### Textbook: Reinforcement Learning: An Introduction - Sutton and Barto

# Chapter 3. Finite Markov Decision Process (MDP)
> _MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards._

+ _Agent_: Learner and decision maker

+ _Environment_: Thing the agent interacts with, comprising evertything outside it.

![](/Figs/RL_Sutton/Ch3/Interaction.jpg){: width="100%" height="100%"}

+ MDP depends only on the preceding state and action. 

$${\small p(s',r|s,a)=\Pr\{S_{t}=s',R_{t}=r|S_{t-1}=s,A_{t-1}=a\} }$$

+ _State-transition probabilities_

$${\small  p(s'|s,a)=\Pr\{S_{t}=s'|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s',r|s,a) }$$

+ Expected rewards for state-action pairs

$${\small  r(s,a)=\mathbb{E}[R_{t}|S_{t-1}=s,A_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a) }$$

+ Expected rewards for state-action-next-state triples

$${\small  r(s,a,s')=\mathbb{E}[R_{t}|S_{t-1}=s,A_{t-1}=a,S_{t}=s']=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)} }$$

+ Goal: Reward hypothesis

> _That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)._

+ Return for episode with terminal state at T

$${\small G_{t} = R_{t+1}+R_{t+2}+\cdots+R_{T} }$$

+ Discounted Return for continuous tasks with discount rate of $\gamma$

$${\small G_{t} = R_{t+1}+{\gamma}R_{t+2}+\gamma^{2}R_{t+2}+\cdots = \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1} }$$

+ Unified Return

$${\small G_{t} = \sum_{k=t+1}^{T}\gamma^{k-t-1}R_{k} }$$

+ Policy (${\pi}(a\|s)$): A mapping from states to probabilities of selecting each possible action

+ State-value fuction ($v_{\pi}(s)$)

$${\small v_{\pi}(s) = \mathbb{E_{\pi}}[G_{t}|S_{t}=s] }$$

+ Action-value fuction ($q_{\pi}(s,a)$)

$${\small q_{\pi}(s,a) = \mathbb{E_{\pi}}[G_{t}|S_{t}=s,A_{t}=a] }$$

+ Bellman equation for $v_{\pi}(s)$

$${\small v_{\pi}(s)=\mathbb{E_{\pi}}[G_{t}|S_{t}=s]=\mathbb{E_{\pi}}[R_{t}+{\gamma}G_{t+1}|S_{t}=s]  }\\
 {\small            =\sum_{a}\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma\mathbb{E_{\pi}}[G_{t+1}|S_{t+1}=s']]  }\\
 {\small            =\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+{\gamma}v_{\pi}(s')] }
$$

+ Bellman equation for $q_{\pi}(s,a)$

$${\small q_{\pi}(s,a)=\mathbb{E_{\pi}}[G_{t}|S_{t}=s, A_{t}=a] }\\
{\small =\mathbb{E_{\pi}}[R_{t}+{\gamma}G_{t+1}|S_{t}=s, A_{t}=a]  }\\
 {\small            =\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma\mathbb{E_{\pi}}[G_{t+1}|S_{t+1}=s']]  }\\
 {\small            =\sum_{s',r}p(s',r|s,a)[r+{\gamma}v_{\pi}(s')] }
$$

+ Optimal Policy

> _A policy $\pi$ is defined to be better than or equal to a policy $\pi'$ if its expected return is greater than or equal to that of $\pi'$ for all states. In other words,_ $\pi{\geq}\pi'$ _if and only if_ $v_{\pi}(s){\geq}v_{\pi'}(s)$ _for all_ $s\in\mathcal{S}$. _There is always at leat one policy that is better than or equal to all other policies. This is an optimal policy._

```ruby
state, reward = Environment(action)
action = agent(state, reward, goal)
```

