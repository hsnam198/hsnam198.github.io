---
layout: post
title: Reinforcement Learning Study 3
date: 2021-02-07 17:00:00 +0900
category: Study 
---
#### Textbook: Reinforcement Learning: An Introduction - Sutton and Barto

# Chapter 4. Dynamic Programming
> _The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environement as a MDP._

+ Policy Evaluation (Prediction): To compute the state-value function for an arbitrary policy

_Iterative policy evaluation_

$$
{\small v_{k+1}(s) = \mathbb{E_{\pi}}[R_{t+1}+{\gamma}v_{k}(S_{t+1})|S_{t}=s] }\\
{\small = \sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+{\gamma}v_{k}(s')]}
$$

```ruby
def prob_policy(a,s):
 return prob

def prob_s_r(_s,r,s,a):
 return prob
 
v = zeros()
delta = 0

while True:
 oldV = v(s)
 for s in S:
  oldV = v(s)
  _vs = 0
   for a in A:
    _vsa = 0
    for _s in S:
     for r in R:
      _vsa += prob_s_r(_s,r,s,a)*(r+gamma*v(_s)
    _vs += prob_policy(a,s)*_vsa 
    
  v(s) = _vs
  delta = max(delta, abs(oldV-v(s))
  
  if (delta < th):
   break
   
 finalV = v
```

+ Policy Improvement

$$
{\small \pi'(s) = \underset{a}{\arg\max}q_{\pi}(s,a)
$$

